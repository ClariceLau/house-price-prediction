---
title: "Untitled"
output: html_document
date: "2023-05-20"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(arules)
library(caret)
library(catboost)
library(cluster)
library(dplyr)
library(e1071)
library(factoextra)
library(ggplot2)
library(glmnet)
library(lattice)
library(lightgbm)
library(randomForest)
library(tidyr)
library(viridis)
library(xgboost)
library(Metrics)
library(Rtsne)
```

```{r}
source("data_cleaning.r")
```

## Data Transformation/Scaling

Split the numerical variables into features and the target variable.

```{r}
X_num <- subset(numerical_data, select = -c(SalePrice))
y <- subset(numerical_data, select = c(SalePrice))$SalePrice
```

Log Transformation for numerical features.

```{r}
skewness_before <- sapply(X_num, function(x) {
  e1071::skewness(x)
})

X_num_skewed <- skewness_before[abs(skewness_before) > 0.75]

for (x in names(X_num_skewed)) {
  # bc <- BoxCoxTrans(X_num[[x]], lambda = 0.15)
  # X_num[[x]] <- predict(bc, X_num[[x]])
  X_num[[x]] <- log1p(X_num[[x]])
}

skewness_after <- sapply(X_num, function(x) {
  e1071::skewness(x)
})

data.frame(skewness_before, skewness_after)
```

Log Transformation for the target variable.

```{r}
skewness_before <- e1071::skewness(y)

y_t <- log1p(y)

skewness_after <- e1071::skewness(y_t)

sprintf("Before: %f, After: %f", skewness_before, skewness_after)
```

Scaling

```{r}
X_num <- scale(X_num)
```

One-Hot Encoding for categorical variables.

```{r}
encoder <- dummyVars(~., data = categorical_data)

X_cat <- predict(encoder, newdata = categorical_data)
X_cat <- data.frame(X_cat)
```

Split into train and validation sets.

```{r}
X <- cbind(X_cat, X_num)

train_idx <- createDataPartition(y_t, p = 0.7, list = F)
X_train <- X[train_idx, ]
y_train <- y_t[train_idx]

X_val <- X[-train_idx, ]
y_val <- y_t[-train_idx]
```

```{r}
dim(X)
length(y)

dim(X_train)
length(y_train)

dim(X_val)
length(y_val)

names(X)
```

## Clustering

### PCA
```{r}
# Load the necessary libraries
library(stats)
library(factoextra)

# Perform PCA on the data matrix
pca_result <- prcomp(X)

# Extract the principal components
principal_components <- as.data.frame(pca_result$x)

# Determine the optimal number of clusters using the elbow method
fviz_nbclust(principal_components, kmeans, method = "wss")

k <- 3
# Perform K-means clustering on the principal components
kmeans_result <- kmeans(principal_components, centers = k)
cluster_assignments <- kmeans_result$cluster
print(cluster_assignments)

# Add the cluster assignments to the principal components data
principal_components$cluster <- as.factor(cluster_assignments)

ggplot(principal_components, aes(PC1, PC2, color = cluster)) +
  geom_point() +
  labs(x = "Principal Component 1", y = "Principal Component 2") +
  scale_color_discrete(name = "Cluster") +
 theme_minimal()
```

### t-SNE

```{r}
tsne <- Rtsne(X)
tsne_df <- data.frame(tsne)
```

```{r}
dist_mat <- dist(tsne$Y, method = "euclidean")
hclust_avg <- hclust(dist_mat, method = "average")

dend <- as.dendrogram(hclust_avg)
plot(dend)
```

```{r}
k = 15
cut_avg <- cutree(hclust_avg, k)
tsne_df$cluster <- cut_avg

getCentroid <- function(points) {
  xy <- numeric(2)
  
  xy[1] = mean(points[, 1])
  xy[2] = mean(points[, 2])

  return(xy)
}

centroids = matrix(0, k, 2)
for (i in unique(cut_avg)) centroids[i, ] <- getCentroid(tsne$Y[cut_avg == i,])
```

```{r}
tsne_df

ggplot(tsne_df, aes(x=Y.1, y=Y.2, color=y)) +
  geom_point() +
  scale_color_gradientn(colours = heat.colors(10))

ggplot(data.frame(table(tsne_df$cluster)), aes(x=Var1, y=Freq)) +
  geom_bar(stat = "identity") +
  coord_flip()

ggplot(tsne_df, aes(x=Y.1, y=Y.2, color=cluster)) +
  geom_point() +
  scale_color_viridis() +
  scale_fill_viridis(discrete = T) +
  geom_point(data = data.frame(centroids), aes(x=X1, y=X2), color="black", fill="white", shape=21, size=8) +
  geom_text(data = data.frame(centroids), aes(x=X1, y=X2, label=1:k), color="black")

fviz_silhouette(silhouette(cutree(hclust_avg, k = k), dist_mat))

```

## Baselines

```{r}
baselines_rmse <- list()
actual_metrics <- list()

metrics_fusion <- function(y_pred, y) {
  y_pred_inv <- expm1(y_pred)
  y_inv <- expm1(y)

  a <- mae(y_pred_inv, y_inv)
  b <- mape(y_pred_inv, y_inv)
  c <- rmse(y_pred_inv, y_inv)
  d <- mse(y_pred_inv, y_inv)
  e <- R2(y_pred_inv, y_inv)

  return(c("mae" = a, "mape" = b, "rmse" = c, "mse" = d, "r2" = e))
}
```

### Linear Regression

```{r}
linreg <- lm(SalePrice ~ ., data = cbind(X_train, SalePrice = y_train))
y_pred <- predict(linreg, newdata = X_val)

score <- rmse(y_pred, y_val)
baselines_rmse$linear_regression = score
score
actual_metrics$linear_regression <- metrics_fusion(y_pred, y_val)
```

### Lasso Regression

```{r}
lasso <- cv.glmnet(x = as.matrix(X_train), y = y_train, alpha = 1)
y_pred <- predict(lasso, newx = as.matrix(X_val))

score <- rmse(y_pred, y_val)
baselines_rmse$lasso <- score
score
actual_metrics$lasso <- metrics_fusion(y_pred, y_val)
```

### Ridge Regression

```{r}
ridge <- cv.glmnet(x = as.matrix(X_train), y = y_train, alpha = 0)
y_pred <- predict(ridge, newx = as.matrix(X_val))

score <- rmse(y_pred, y_val)
baselines_rmse$ridge <- score
score
actual_metrics$ridge <- metrics_fusion(y_pred, y_val)
```

### Elastic Net

```{r}
results <- data.frame()

for (i in 0:20) {
  elasticnet <- cv.glmnet(x = as.matrix(X_train), y = y_train, alpha = i/20)
  y_pred <- predict(elasticnet, newx = as.matrix(X_val))

  row <- data.frame(alpha = i/20, rmse = rmse(y_pred, y_val))
  results <- rbind(results, row)
}

score <- min(results$rmse)
baselines_rmse$elasticnet = score
score
actual_metrics$elasticnet <- metrics_fusion(y_pred, y_val)
```

### K-Nearest Neighbors Regression

```{r}
knn <- knnreg(x = X_train, y = y_train)
y_pred <- predict(knn, newdata = X_val)

score <- rmse(y_pred, y_val)
baselines_rmse$knn <- score
score
actual_metrics$knn <- metrics_fusion(y_pred, y_val)
```

### Support Vector Regression

```{r}
svr <- e1071::svm(SalePrice ~ ., data = cbind(X_train, SalePrice = y_train))
y_pred <- predict(svr, newdata = X_val)

score <- rmse(y_pred, y_val)
baselines_rmse$svr <- score
score
actual_metrics$svr <- metrics_fusion(y_pred, y_val)
```

### Decision Tree

```{r}
dt = caret::train(x = X_train, y = y_train, method="rpart")
y_pred = predict(dt, newdata = X_val)

score <- rmse(y_pred, y_val)
baselines_rmse$decision_tree <- score
score
actual_metrics$decision_tree <- metrics_fusion(y_pred, y_val)
```

## Ensemble Learning

```{r}
ensemble_rmse <- list()
ensemble_actual_metrics <- list()
```

### Random Forest

```{r}
rf <- randomForest(x = X_train, y = y_train, ntree = 1000, proximity = T)
y_pred = predict(rf, newdata = X_val)

score <- rmse(y_pred, y_val)
ensemble_rmse$random_forest <- score
score
ensemble_actual_metrics$random_forest <- metrics_fusion(y_pred, y_val)
```

```{r}
varImpPlot(rf)
```

### Gradient-Boosted Trees

```{r}
dtrain <- xgb.DMatrix(data = as.matrix(X_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(X_val), label = y_val)

xgb <- xgboost(
  booster="gbtree",
  data = dtrain,
  nrounds = 2500,
  verbose = FALSE,
  eval_metric = "rmse",
  nthread = 8,
  eta = 0.01,
  gamma = 0.0468,
  max_depth = 6,
  min_child_weight = 1.41,
  subsample = 0.769,
  colsample_bytree = 0.283
)
y_pred <- predict(xgb, newdata = dtest)

score <- rmse(y_pred, y_val)
ensemble_rmse$xgboost <- score
score
ensemble_actual_metrics$xgboost <- metrics_fusion(y_pred, y_val)
```

```{r}
lgb_train <- lgb.Dataset(data = as.matrix(X_train), label = y_train)
lgb_test <- lgb.Dataset(data = as.matrix(X_val), label = y_val)

params <- list(
  objective = "regression",
  metric = "rmse",
  boosting_type = "gbdt",
  num_boost_round = 100,
  num_leaves = 31,
  learning_rate = 0.1,
  feature_fraction = 0.9,
  bagging_fraction = 0.8,
  bagging_freq = 5
)

lgb <- lgb.train(
  params = params,
  data = lgb_train,
  valids = list(test = lgb_test),
  early_stopping_rounds = 10,
  verbose = 0
)

y_pred <- predict(lgb, data = as.matrix(X_val))

score <- rmse(y_pred, y_val)
ensemble_rmse$lightgbm <- score
score
ensemble_actual_metrics$lightgbm <- metrics_fusion(y_pred, y_val)
```

```{r}
learn_pool <- catboost.load_pool(data = X_train, label = y_train)
test_pool <- catboost.load_pool(data = X_val, label = y_val)

params <- list(
  loss_function = "RMSE",
  iterations = 100,
  learning_rate = 0.1,
  depth = 10
)

catb <- catboost.train(
  params = params,
  learn_pool = learn_pool,
  test_pool = test_pool
)

y_pred <- catboost.predict(catb, test_pool)

score <- rmse(y_pred, y_val)
ensemble_rmse$catboost <- score
score
ensemble_actual_metrics$catboost <- metrics_fusion(y_pred, y_val)
```

## Association Rule Mining on Housing Characteristics

```{r}
transactions <- transactions(categorical_data)
summary(transactions)
```

```{r}
inspect(head(transactions, n = 1))
```

```{r}
rules <- apriori(transactions, parameter = list(support = 0.1, confidence = 0.8))
```

```{r}
summary(rules)
```

```{r}
inspect(head(rules, n = 3, by = "lift"))
```

### Comparison

```{r}
# Convert baselines_rmse list to a data frame
print(baselines_rmse)
df <- data.frame(models = names(baselines_rmse), rmse = unlist(baselines_rmse))
# Create the bar chart using ggplot
ggplot(df, aes(x = models, y = rmse)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  xlab("Models") +
  ylab("RMSE") +
  ggtitle("Baseline RMSE") +
  theme_minimal()

print(ensemble_rmse)
df <- data.frame(models = names(ensemble_rmse), rmse = unlist(ensemble_rmse))
# Create the bar chart using ggplot
ggplot(df, aes(x = models, y = rmse)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  xlab("Models") +
  ylab("RMSE") +
  ggtitle("Ensemble RMSE") +
  theme_minimal()
```

```{r}
data.frame(t(data.frame(actual_metrics))) %>% arrange(desc(r2))
```

```{r}
data.frame(t(data.frame(ensemble_actual_metrics))) %>% arrange(desc(r2))
```